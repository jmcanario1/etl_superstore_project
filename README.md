# ğŸ› ï¸ Projeto de Estudo: Pipeline ETL com Python

Este projeto foi criado com o objetivo de praticar e aprender sobre **ETL (Extract, Transform, Load)** utilizando Python e pandas, com base em um dataset de vendas similar ao Superstore (contendo informaÃ§Ãµes de pedidos, clientes, produtos, lucros, descontos, etc).

---

## ğŸ“Œ Objetivo

O objetivo principal Ã© construir um **pipeline ETL simples e bem estruturado** que possa:

- Extrair dados de um arquivo CSV
- Transformar os dados brutos (limpeza, enriquecimento, novas colunas)
- Salvar os dados limpos e prontos para anÃ¡lise

Este projeto Ã© **educacional**, ideal para quem estÃ¡ comeÃ§ando a aprender sobre **Data Engineering**, **Data Science**, ou **AnÃ¡lise de Dados com Python**.

---

## ğŸ“ Estrutura do Projeto

```
etl_superstore_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ etl_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Tecnologias Utilizadas

- Python 3.9+
- Pandas
- (opcional) SQLite ou Parquet (para etapas futuras)

---

## ğŸš€ Como Executar

1. **Clone o repositÃ³rio**

git clone https://github.com/jmcanario1/etl_superstore_project.git
cd etl_superstore_project


2. **Crie um ambiente virtual (opcional, mas recomendado)**

python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate no Windows


3. **Instale as dependÃªncias**

pip install -r requirements.txt


4. **Coloque o arquivo de dados em data/raw/superstore.csv**


5. **Execute o pipeline**

python etl_pipeline.py


ğŸ”„ Funcionalidades do Pipeline
ConversÃ£o de datas (Order Date, Ship Date)

CriaÃ§Ã£o de colunas derivadas:

Tempo de envio (em dias)

Margem de lucro

ClassificaÃ§Ã£o de lucro (Alto, Moderado, Negativo)

Tratamento de colunas (snake_case)

RemoÃ§Ã£o de dados invÃ¡lidos (ex: entregas com tempo negativo)

Salvamento do dataset limpo em CSV

ğŸ“ˆ PrÃ³ximas Melhorias
Salvar os dados transformados em formato Parquet ou banco de dados (SQLite/PostgreSQL)

Adicionar testes automatizados (pytest)

Criar um dashboard com Streamlit ou Dash

Automatizar a execuÃ§Ã£o com Airflow ou Prefect

ğŸ§  Aprendizados
Este projeto Ã© parte da minha jornada de aprendizado em:

Engenharia de dados

Processamento de dados com Python

EstruturaÃ§Ã£o de pipelines ETL

Boas prÃ¡ticas em projetos de dados

ğŸ“¬ Contato
Caso queira trocar ideias, sugestÃµes ou dÃºvidas:

Nome: [JoÃ£o Marcelo CanÃ¡rio]

Email: [joaomarcelocanariodev@gmail.com]

LinkedIn: [https://www.linkedin.com/in/joaomarcelocanario/]

Nota: Este projeto Ã© apenas para fins educacionais e nÃ£o deve ser usado em produÃ§Ã£o sem adaptaÃ§Ãµes apropriadas.